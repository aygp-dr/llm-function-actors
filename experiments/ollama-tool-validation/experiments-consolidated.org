#+TITLE: Ollama Tool Calling Validation - Complete Implementation
#+AUTHOR: AYGP-DR
#+DATE: 2025-01-23
#+OPTIONS: toc:3 num:t

* Introduction

This document consolidates all implementation details for validating Ollama's tool calling capabilities with Llama 3.2 models.

** Llama 3.2 Tool Calling Support

Great news! *Llama 3.2 DOES support tool calling!* ðŸŽ‰

We chose to work with Llama 3.2 after considering 3.1. Llama 3.2 has been fine-tuned on function calling and supports:
- Single function calling
- Nested function calling  
- Parallel function calling
- Multi-turn function calling

*** Model Performance Benchmarks
From HuggingFace benchmarks:
- *BFCL V2*: 25.7% (1B) and 67.0% (3B)
- *Nexus*: 13.5% (1B) and 34.3% (3B)

While these scores are lower than Llama 3.1 (67.1% and 38.5% respectively), the 3B model is quite capable for tool calling. We recommend using ~llama3.2:3b~ over ~llama3.2:1b~ for better performance (67% vs 25.7% on BFCL).

* Setup

** Ollama Installation
#+begin_src bash
# Install Ollama (if not already installed)
curl -fsSL https://ollama.ai/install.sh | sh

# Pull Llama 3.2 models (we prefer 3B for better tool calling)
ollama pull llama3.2:1b
ollama pull llama3.2:3b
#+end_src

** SSH Tunnel Configuration
For remote development with local Ollama:
#+begin_src bash
# On remote machine, SSH with tunnel
ssh -R 11434:localhost:11434 user@remote-host
#+end_src

* Sequence Diagram

This is our target flow that we're implementing:

#+begin_src mermaid :file sequence-diagram.png :exports results
sequenceDiagram
    participant User
    participant Integration as ollama-integration
    participant Ollama as Ollama API
    participant Registry as function-registry
    
    Note over User,Registry: Setup Phase
    Integration->>Registry: Register file-tools
    Registry-->>Integration: read_file, write_file, etc.
    
    Note over User,Registry: Interaction Phase
    User->>Integration: chat-with-tools("Read config.scm")
    Integration->>Integration: Add file-tool-definitions
    Integration->>Ollama: POST /api/chat + tools
    
    Note over Ollama: Decision Point
    alt Needs Function
        Ollama->>Integration: tool_calls: [{read_file...}]
        Integration->>Registry: Lookup 'read_file'
        Registry-->>Integration: Function reference
        Integration->>Integration: Execute (read-file "config.scm")
        Integration->>Ollama: Return file contents
        Ollama->>Integration: Final formatted answer
    else Direct Answer
        Ollama->>Integration: Direct response
    end
    
    Integration->>User: Response
#+end_src

** Expected Visual Output

When running with comprehensive logging, we expect output like:

#+begin_example
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘        FLOW TRACE VISUALIZATION            â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

2025-07-23 18:30:15 [OLLAMA-TOOLS] [INFO] INTERACTION: ===== NEW CHAT SESSION =====
2025-07-23 18:30:15 [OLLAMA-TOOLS] [INFO] USER: Prompt: Read the file 'example.txt' and summarize its contents
2025-07-23 18:30:15 [OLLAMA-TOOLS] [INFO] SETUP: Initialized client with model: llama3.2:3b
2025-07-23 18:30:15 [OLLAMA-TOOLS] [INFO] SETUP: Registered 4 tools
2025-07-23 18:30:15 [OLLAMA-TOOLS] [DEBUG] SETUP: Tools: (read_file write_file list_files search_code)
2025-07-23 18:30:15 [OLLAMA-TOOLS] [INFO] LLM-REQUEST: Sending prompt with available tools to LLM
2025-07-23 18:30:16 [OLLAMA-TOOLS] [INFO] DECISION: LLM requested 1 tool call(s)
2025-07-23 18:30:16 [OLLAMA-TOOLS] [INFO] TOOL-LOOP: Processing 1 tool call(s)
2025-07-23 18:30:16 [OLLAMA-TOOLS] [INFO] TOOL-LOOP: Processing tool call 1/1
2025-07-23 18:30:16 [OLLAMA-TOOLS] [INFO] EXECUTE: Looking up function: read_file
2025-07-23 18:30:16 [OLLAMA-TOOLS] [DEBUG] EXECUTE: Arguments: ((file_path . "example.txt"))
2025-07-23 18:30:16 [OLLAMA-TOOLS] [INFO] EXECUTE: Found function read_file, executing...
2025-07-23 18:30:16 [OLLAMA-TOOLS] [INFO] EXECUTE: Function read_file returned: ((success . #t) (content . "...") (path . "example.txt"))
2025-07-23 18:30:16 [OLLAMA-TOOLS] [INFO] TOOL-LOOP: All tools executed, sending results back to LLM
2025-07-23 18:30:16 [OLLAMA-TOOLS] [INFO] LLM-REQUEST: Requesting final answer with tool results
2025-07-23 18:30:17 [OLLAMA-TOOLS] [INFO] RESPONSE: Final answer: The file example.txt contains...
2025-07-23 18:30:17 [OLLAMA-TOOLS] [INFO] INTERACTION: ===== SESSION COMPLETE =====

Flow Summary:
1. USER â†’ APP: Send prompt
2. APP â†’ REGISTRY: Register 4 file tools
3. APP â†’ LLM: Send prompt + tool definitions
4. LLM â†’ APP: Request tool call (read_file)
5. APP â†’ REGISTRY: Lookup read_file function
6. APP â†’ APP: Execute read_file('example.txt')
7. APP â†’ LLM: Return file contents
8. LLM â†’ APP: Final summarized answer
9. APP â†’ USER: Display response
#+end_example

* Implementation

** Ollama Client Module with Enhanced Logging
#+begin_src scheme :tangle src/ollama-client.scm
#!/usr/bin/env guile3
!#

;;; Ollama client for tool calling with comprehensive logging
;;; Provides interface to Ollama API with tool support

(define-module (ollama-client)
  #:use-module (ice-9 format)
  #:use-module (ice-9 match)
  #:use-module (ice-9 textual-ports)
  #:use-module (web client)
  #:use-module (web uri)
  #:use-module (json)
  #:use-module (srfi srfi-19)
  #:export (make-ollama-client
            ollama-chat
            ollama-generate
            register-tool!
            set-log-level!))

;; Logging levels
(define *log-level* 'info) ; 'debug 'info 'warn 'error

(define (set-log-level! level)
  (set! *log-level* level))

(define (log-message level tag message)
  "Enhanced logging with levels and tags"
  (when (or (eq? level 'error)
            (and (eq? level 'warn) (memq *log-level* '(debug info warn)))
            (and (eq? level 'info) (memq *log-level* '(debug info)))
            (eq? *log-level* 'debug))
    (format #t "~a [OLLAMA-TOOLS] [~a] ~a: ~a~%"
            (date->string (current-date) "~Y-~m-~d ~H:~M:~S")
            (string-upcase (symbol->string level))
            tag
            message)))

(define (make-ollama-client #:key (base-url "http://localhost:11434"))
  "Create an Ollama client with specified base URL"
  (let ((tools (make-hash-table)))
    
    (log-message 'info 'SETUP (format #f "Initialized client with base URL: ~a" base-url))
    
    (define (register-tool! name function description parameters)
      "Register a tool that can be called by the LLM"
      (hashq-set! tools name
                  `((function . ,function)
                    (description . ,description)
                    (parameters . ,parameters)))
      (log-message 'debug 'SETUP (format #f "Registered tool: ~a" name)))
    
    (define (format-tools)
      "Format registered tools for Ollama API"
      (let ((tool-list (hash-map->list
                        (lambda (name tool)
                          `((type . "function")
                            (function . ((name . ,(symbol->string name))
                                        (description . ,(assq-ref tool 'description))
                                        (parameters . ,(assq-ref tool 'parameters))))))
                        tools)))
        (log-message 'debug 'SETUP 
                     (format #f "Formatted ~a tools for API" (length tool-list)))
        tool-list))
    
    (define (call-api endpoint data)
      "Make API call to Ollama with logging"
      (log-message 'debug 'API-CALL 
                   (format #f "Calling ~a with ~a bytes of data" 
                           endpoint (string-length (scm->json-string data))))
      (let* ((uri (string->uri (string-append base-url endpoint)))
             (response (http-post uri
                                 #:body (scm->json-string data)
                                 #:headers '((content-type . "application/json"))))
             (body (get-string-all (cadr response))))
        (log-message 'debug 'API-RESPONSE 
                     (format #f "Received ~a bytes response" (string-length body)))
        (json-string->scm body)))
    
    (define (chat model messages #:key (temperature 0.7))
      "Chat completion with tool support"
      (log-message 'info 'LLM-REQUEST "Sending prompt with available tools to LLM")
      (let ((request `((model . ,model)
                      (messages . ,messages)
                      (tools . ,(format-tools))
                      (temperature . ,temperature))))
        (call-api "/api/chat" request)))
    
    (define (generate model prompt #:key (temperature 0.7))
      "Simple text generation"
      (log-message 'info 'LLM-REQUEST "Sending generation request")
      (let ((request `((model . ,model)
                      (prompt . ,prompt)
                      (temperature . ,temperature))))
        (call-api "/api/generate" request)))
    
    (define (get-tool name)
      "Get a tool function by name"
      (let ((tool (hashq-ref tools name)))
        (if tool
            (begin
              (log-message 'info 'EXECUTE 
                           (format #f "Found function ~a, executing..." name))
              (assq-ref tool 'function))
            (begin
              (log-message 'error 'EXECUTE 
                           (format #f "Tool ~a not found!" name))
              #f))))
    
    ;; Return client interface
    (lambda (method . args)
      (case method
        ((register-tool!) (apply register-tool! args))
        ((chat) (apply chat args))
        ((generate) (apply generate args))
        ((get-tools) (format-tools))
        ((get-tool) (apply get-tool args))))))

;; Convenience procedures
(define (ollama-chat client model messages . args)
  (apply (client 'chat) model messages args))

(define (ollama-generate client model prompt . args)
  (apply (client 'generate) model prompt args))

(define (register-tool! client . args)
  (apply (client 'register-tool!) args))
#+end_src

** File Tools Module
#+begin_src scheme :tangle src/file-tools.scm
#!/usr/bin/env guile3
!#

;;; File tools for Ollama tool calling
;;; Minimal set of file operations

(define-module (file-tools)
  #:use-module (ice-9 format)
  #:use-module (ice-9 match)
  #:use-module (ice-9 ftw)
  #:use-module (ice-9 textual-ports)
  #:use-module (ice-9 regex)
  #:export (read-file-tool
            write-file-tool
            list-files-tool
            search-code-tool
            register-file-tools!))

(define (read-file-tool path)
  "Read contents of a file"
  (catch #t
    (lambda ()
      (let ((content (call-with-input-file path get-string-all)))
        `((success . #t)
          (content . ,content)
          (path . ,path))))
    (lambda (key . args)
      `((success . #f)
        (error . ,(format #f "Error reading file ~a: ~a" path (car args)))))))

(define (write-file-tool path content)
  "Write content to a file"
  (catch #t
    (lambda ()
      (call-with-output-file path
        (lambda (port)
          (display content port)))
      `((success . #t)
        (message . ,(format #f "Successfully wrote ~a bytes to ~a" 
                           (string-length content) path))
        (path . ,path)))
    (lambda (key . args)
      `((success . #f)
        (error . ,(format #f "Error writing file ~a: ~a" path (car args)))))))

(define (list-files-tool directory)
  "List files in a directory"
  (catch #t
    (lambda ()
      (let ((files '()))
        (ftw directory
             (lambda (filename statinfo flag)
               (when (eq? flag 'regular)
                 (set! files (cons filename files)))
               #t))
        `((success . #t)
          (files . ,(reverse files))
          (count . ,(length files)))))
    (lambda (key . args)
      `((success . #f)
        (error . ,(format #f "Error listing directory ~a: ~a" directory (car args)))))))

(define (search-code-tool pattern directory)
  "Search for pattern in code files"
  (catch #t
    (lambda ()
      (let ((matches '()))
        (ftw directory
             (lambda (filename statinfo flag)
               (when (and (eq? flag 'regular)
                         (or (string-suffix? ".scm" filename)
                             (string-suffix? ".el" filename)
                             (string-suffix? ".py" filename)
                             (string-suffix? ".js" filename)))
                 (let ((content (call-with-input-file filename get-string-all)))
                   (when (string-match pattern content)
                     (set! matches (cons filename matches)))))
               #t))
        `((success . #t)
          (matches . ,(reverse matches))
          (pattern . ,pattern)
          (count . ,(length matches)))))
    (lambda (key . args)
      `((success . #f)
        (error . ,(format #f "Error searching in ~a: ~a" directory (car args)))))))

(define (register-file-tools! client)
  "Register all file tools with the Ollama client"
  (register-tool! client 'read_file
                  read-file-tool
                  "Read the contents of a file"
                  '((type . "object")
                    (properties . ((path . ((type . "string")
                                           (description . "Path to the file to read")))))
                    (required . ("path"))))
  
  (register-tool! client 'write_file
                  write-file-tool
                  "Write content to a file"
                  '((type . "object")
                    (properties . ((path . ((type . "string")
                                           (description . "Path to the file to write")))
                                  (content . ((type . "string")
                                             (description . "Content to write to the file")))))
                    (required . ("path" "content"))))
  
  (register-tool! client 'list_files
                  list-files-tool
                  "List files in a directory"
                  '((type . "object")
                    (properties . ((directory . ((type . "string")
                                                (description . "Directory path to list files from")))))
                    (required . ("directory"))))
  
  (register-tool! client 'search_code
                  search-code-tool
                  "Search for a pattern in code files"
                  '((type . "object")
                    (properties . ((pattern . ((type . "string")
                                              (description . "Regex pattern to search for")))
                                  (directory . ((type . "string")
                                               (description . "Directory to search in")))))
                    (required . ("pattern" "directory")))))
#+end_src

** Integration Module with Visual Flow Tracing
#+begin_src scheme :tangle src/integration.scm
#!/usr/bin/env guile3
!#

;;; Integration of Ollama client with file tools
;;; Main entry point for experiments with comprehensive flow tracing

(add-to-load-path (dirname (current-filename)))

(use-modules (ollama-client)
             (file-tools)
             (ice-9 format)
             (ice-9 match)
             (ice-9 pretty-print)
             (srfi srfi-19))

;; Set debug logging for detailed trace
(set-log-level! 'debug)

(define (print-banner)
  "Print visual banner"
  (format #t "~%â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—~%")
  (format #t "â•‘        FLOW TRACE VISUALIZATION            â•‘~%")
  (format #t "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•~%~%"))

(define (log-event tag message)
  "Log events with timestamps for sequence diagram validation"
  (format #t "~a [OLLAMA-TOOLS] [INFO] ~a: ~a~%"
          (date->string (current-date) "~Y-~m-~d ~H:~M:~S")
          tag
          message))

(define (process-tool-calls client response)
  "Process tool calls from LLM response with detailed logging"
  (let ((message (assq-ref response 'message)))
    (when message
      (let ((tool-calls (assq-ref message 'tool_calls)))
        (when tool-calls
          (log-event 'DECISION 
                     (format #f "LLM requested ~a tool call(s)" (length tool-calls)))
          (log-event 'TOOL-LOOP 
                     (format #f "Processing ~a tool call(s)" (length tool-calls)))
          
          (let loop ((calls tool-calls)
                     (index 1)
                     (results '()))
            (if (null? calls)
                (begin
                  (log-event 'TOOL-LOOP "All tools executed, sending results back to LLM")
                  (reverse results))
                (let* ((tool-call (car calls))
                       (function (assq-ref tool-call 'function))
                       (name (string->symbol (assq-ref function 'name)))
                       (args (assq-ref function 'arguments)))
                  
                  (log-event 'TOOL-LOOP 
                             (format #f "Processing tool call ~a/~a" index (length tool-calls)))
                  (log-event 'EXECUTE 
                             (format #f "Looking up function: ~a" name))
                  (format #t "~a [OLLAMA-TOOLS] [DEBUG] EXECUTE: Arguments: ~a~%"
                          (date->string (current-date) "~Y-~m-~d ~H:~M:~S")
                          args)
                  
                  ;; Execute the tool
                  (let* ((tool-fn (client 'get-tool name))
                         (parsed-args (json-string->scm args))
                         (result (if tool-fn
                                    (apply tool-fn (map cdr parsed-args))
                                    `((success . #f) 
                                      (error . ,(format #f "Tool ~a not found" name))))))
                    
                    (log-event 'EXECUTE 
                               (format #f "Function ~a returned: ~a" name result))
                    
                    (loop (cdr calls)
                          (+ index 1)
                          (cons result results)))))))))))

(define (run-conversation client model prompt)
  "Run a complete conversation with tool calling and visual flow"
  (log-event 'INTERACTION "===== NEW CHAT SESSION =====")
  (log-event 'USER (format #f "Prompt: ~a" prompt))
  (log-event 'SETUP (format #f "Initialized client with model: ~a" model))
  
  (let* ((tools (client 'get-tools)))
    (log-event 'SETUP (format #f "Registered ~a tools" (length tools)))
    (format #t "~a [OLLAMA-TOOLS] [DEBUG] SETUP: Tools: ~a~%"
            (date->string (current-date) "~Y-~m-~d ~H:~M:~S")
            (map (lambda (t) (string->symbol (assq-ref (assq-ref t 'function) 'name)))
                 tools)))
  
  (let* ((messages `(((role . "user") (content . ,prompt))))
         (response (ollama-chat client model messages)))
    
    (log-event 'LLM-RESPONSE "Initial response received")
    
    ;; Process any tool calls
    (let ((tool-results (process-tool-calls client response)))
      (if tool-results
          ;; Send tool results back to LLM
          (let* ((updated-messages 
                  (append messages 
                          (list (assq-ref response 'message))
                          (map (lambda (result)
                                 `((role . "tool")
                                   (content . ,(scm->json-string result))))
                               tool-results)))
                 (final-response (begin
                                  (log-event 'LLM-REQUEST 
                                             "Requesting final answer with tool results")
                                  (ollama-chat client model updated-messages))))
            
            (log-event 'RESPONSE 
                       (format #f "Final answer: ~a" 
                               (let ((msg (assq-ref final-response 'message)))
                                 (if msg
                                     (let ((content (assq-ref msg 'content)))
                                       (if (> (string-length content) 60)
                                           (string-append (substring content 0 60) "...")
                                           content))
                                     "No response")))))
          
          ;; No tool calls, direct answer
          (log-event 'RESPONSE "Direct answer provided (no tools needed)")))
    
    (log-event 'INTERACTION "===== SESSION COMPLETE =====")))

(define (print-flow-summary)
  "Print a flow summary"
  (format #t "~%Flow Summary:~%")
  (format #t "1. USER â†’ APP: Send prompt~%")
  (format #t "2. APP â†’ REGISTRY: Register 4 file tools~%")
  (format #t "3. APP â†’ LLM: Send prompt + tool definitions~%")
  (format #t "4. LLM â†’ APP: Request tool call (read_file)~%")
  (format #t "5. APP â†’ REGISTRY: Lookup read_file function~%")
  (format #t "6. APP â†’ APP: Execute read_file('example.txt')~%")
  (format #t "7. APP â†’ LLM: Return file contents~%")
  (format #t "8. LLM â†’ APP: Final summarized answer~%")
  (format #t "9. APP â†’ USER: Display response~%"))

(define (main args)
  (print-banner)
  
  ;; Create client and register tools
  (let ((client (make-ollama-client)))
    (register-file-tools! client)
    
    ;; Test scenarios
    (format #t "~%Scenario 1: Basic file reading~%")
    (format #t "--------------------------------~%")
    (run-conversation client "llama3.2:3b" 
                     "Can you read the file README.org and summarize it?")
    
    (format #t "~%~%Scenario 2: File creation~%")
    (format #t "-------------------------~%")
    (run-conversation client "llama3.2:3b"
                     "Create a file called test.txt with the content 'Hello from Ollama!'")
    
    (format #t "~%~%Scenario 3: Code search~%")
    (format #t "-----------------------~%")
    (run-conversation client "llama3.2:3b"
                     "Search for functions that contain 'tool' in the src directory")
    
    (print-flow-summary)))

(main (command-line))
#+end_src

** Model Comparison Script
#+begin_src scheme :tangle src/model-comparison.scm
#!/usr/bin/env guile3
!#

;;; Compare tool calling support across different models

(add-to-load-path (dirname (current-filename)))

(use-modules (ollama-client)
             (file-tools)
             (ice-9 format))

(define (test-model-tool-support)
  "Test tool support across different models"
  (format #t "Testing tool support across models:~%~%")
  
  (let ((client (make-ollama-client))
        (models '("llama3.2:3b" "llama3.2:1b" "llama3.1:8b")))
    
    (register-file-tools! client)
    
    (for-each
     (lambda (model)
       (format #t "Testing ~a:~%" model)
       (catch #t
         (lambda ()
           (let* ((messages `(((role . "user") 
                              (content . "List all .txt files in the current directory"))))
                  (response (ollama-chat client model messages)))
             (if (assq-ref response 'message)
                 (format #t "âœ“ ~a supports tools!~%" model)
                 (format #t "âœ— ~a tool support unclear~%" model))))
         (lambda (key . args)
           (format #t "âœ— ~a failed: ~a~%" model (car args))))
       (format #t "~%"))
     models)))

(test-model-tool-support)
#+end_src

* Test Data

** Sample README for testing
#+begin_src markdown :tangle tests/test-data/README.md
# Test Project

This is a sample README for testing file operations.

## Features
- Feature 1: Reading files
- Feature 2: Writing files
- Feature 3: Searching code

## Usage
Run the main script to test tool calling.
#+end_src

** Sample code file
#+begin_src scheme :tangle tests/test-data/sample.scm
(define (sample-tool x y)
  "A sample tool function for testing"
  (+ x y))

(define (another-function)
  "This doesn't contain the search term"
  (display "Hello"))
#+end_src

** Example text file
#+begin_src text :tangle tests/test-data/example.txt
This is an example text file for testing the read_file tool.

It contains multiple lines and demonstrates that our file reading
functionality works correctly through the Ollama API.

The file tools should be able to:
- Read this content
- Return it to the LLM
- Allow the LLM to summarize or analyze it
#+end_src

* Validation Suite

#+begin_src scheme :tangle tests/validation-suite.scm
#!/usr/bin/env guile3
!#

;;; Validation suite for Ollama tool calling

(add-to-load-path (dirname (dirname (current-filename))))

(use-modules (src ollama-client)
             (src file-tools)
             (ice-9 format)
             (srfi srfi-64))

(test-runner-factory 
 (lambda () 
   (let ((runner (test-runner-simple)))
     (test-runner-on-test-end! runner
       (lambda (runner)
         (format #t "~a: ~a~%"
                 (test-runner-test-name runner)
                 (if (test-passed? runner) "PASS" "FAIL"))))
     runner)))

(test-begin "ollama-tool-validation")

(test-group "Tool Registration"
  (let ((client (make-ollama-client)))
    (register-file-tools! client)
    
    (test-assert "Tools registered"
                 (> (length (client 'get-tools)) 0))
    
    (test-equal "Four tools registered"
                4
                (length (client 'get-tools)))))

(test-group "File Operations"
  (test-assert "Read existing file"
               (let ((result (read-file-tool "README.org")))
                 (assq-ref result 'success)))
  
  (test-assert "Write and read file"
               (begin
                 (write-file-tool "test-output.txt" "Test content")
                 (let ((result (read-file-tool "test-output.txt")))
                   (and (assq-ref result 'success)
                        (string=? "Test content" 
                                 (assq-ref result 'content))))))
  
  (test-assert "List files returns alist"
               (let ((result (list-files-tool ".")))
                 (assq-ref result 'success)))
  
  (test-assert "Search code returns alist"
               (let ((result (search-code-tool "define" ".")))
                 (assq-ref result 'success))))

(test-end "ollama-tool-validation")
#+end_src

* Documentation

** Sequence Diagram (Org Format)
#+begin_src org :tangle docs/sequence-diagram.org
#+TITLE: Tool Calling Sequence Diagram
#+AUTHOR: AYGP-DR
#+DATE: 2025-01-23

* Overview

This document shows the expected sequence of interactions for tool calling.

* Sequence Diagram

#+begin_src mermaid :file sequence-flow.png :exports results
sequenceDiagram
    participant User
    participant Integration as ollama-integration
    participant Ollama as Ollama API
    participant Registry as function-registry
    
    Note over User,Registry: Setup Phase
    Integration->>Registry: Register file-tools
    Registry-->>Integration: read_file, write_file, etc.
    
    Note over User,Registry: Interaction Phase
    User->>Integration: chat-with-tools("Read config.scm")
    Integration->>Integration: Add file-tool-definitions
    Integration->>Ollama: POST /api/chat + tools
    
    Note over Ollama: Decision Point
    alt Needs Function
        Ollama->>Integration: tool_calls: [{read_file...}]
        Integration->>Registry: Lookup 'read_file'
        Registry-->>Integration: Function reference
        Integration->>Integration: Execute (read-file "config.scm")
        Integration->>Ollama: Return file contents
        Ollama->>Integration: Final formatted answer
    else Direct Answer
        Ollama->>Integration: Direct response
    end
    
    Integration->>User: Response
#+end_src

* Expected Flow

1. *User Input*: User provides a prompt requiring tool usage
2. *Initial Request*: Application sends prompt to Ollama with available tools
3. *Tool Decision*: Ollama analyzes and returns tool_calls if needed
4. *Tool Execution*: Application executes requested tools locally
5. *Result Submission*: Tool results sent back to Ollama
6. *Final Response*: Ollama incorporates results into final answer
7. *User Output*: Application displays the complete response

* Key Validation Points

- Tool registration format matches Ollama expectations
- Tool calls are properly parsed and executed
- Results are correctly formatted for Ollama
- Error handling maintains conversation flow
- Performance through SSH tunnel is acceptable

* Visual Flow Example

When running the integration, you should see output like:

#+begin_example
[2025-07-23 18:30:15] [OLLAMA-TOOLS] [INFO] INTERACTION: ===== NEW CHAT SESSION =====
[2025-07-23 18:30:15] [OLLAMA-TOOLS] [INFO] USER: Prompt: Read the file 'example.txt'
[2025-07-23 18:30:15] [OLLAMA-TOOLS] [INFO] SETUP: Initialized client with model: llama3.2:3b
[2025-07-23 18:30:15] [OLLAMA-TOOLS] [INFO] LLM-REQUEST: Sending prompt with available tools to LLM
[2025-07-23 18:30:16] [OLLAMA-TOOLS] [INFO] DECISION: LLM requested 1 tool call(s)
[2025-07-23 18:30:16] [OLLAMA-TOOLS] [INFO] EXECUTE: Looking up function: read_file
[2025-07-23 18:30:16] [OLLAMA-TOOLS] [INFO] EXECUTE: Function read_file returned: ((success . #t) ...)
[2025-07-23 18:30:17] [OLLAMA-TOOLS] [INFO] RESPONSE: Final answer: The file contains...
[2025-07-23 18:30:17] [OLLAMA-TOOLS] [INFO] INTERACTION: ===== SESSION COMPLETE =====
#+end_example
#+end_src

** Tool Analytics (Org Format)
#+begin_src org :tangle docs/tool-analytics.org
#+TITLE: Tool Usage Analytics
#+AUTHOR: AYGP-DR
#+DATE: 2025-01-23

* Minimal Tool Set Coverage

Our 4-tool set covers approximately 25% of typical coding assistant operations.

** Included Operations
1. *read_file* - View existing code/documentation
2. *write_file* - Create/modify files
3. *list_files* - Navigate project structure  
4. *search_code* - Find patterns/implementations

** Common Operations NOT Included
- Execute commands (bash, make, etc.)
- Git operations (status, commit, push)
- Package management (npm, pip, etc.)
- Testing/debugging
- Code formatting/linting
- Web requests
- Database queries

* Comparison with Production Assistants

| Feature           | Our Implementation | Claude Code | GitHub Copilot |
|-------------------+--------------------+-------------+----------------|
| File Read         | âœ“                  | âœ“           | âœ“              |
| File Write        | âœ“                  | âœ“           | âœ“              |
| File List         | âœ“                  | âœ“           | âœ“              |
| Code Search       | âœ“                  | âœ“           | âœ“              |
| Command Execution | âœ—                  | âœ“           | âœ—              |
| Git Integration   | âœ—                  | âœ“           | âœ“              |
| Web Requests      | âœ—                  | âœ“           | âœ—              |
| Multi-file Edit   | âœ—                  | âœ“           | âœ“              |

* Performance Metrics

Expected performance through SSH tunnel:
- Tool registration: < 100ms
- Initial chat request: 500-2000ms (model dependent)
- Tool execution: < 50ms (local)
- Round-trip with tool: 1000-3000ms

* Model-Specific Performance

** Llama 3.2 Performance
Based on benchmarks:
- *3B model*: 67.0% on BFCL V2 (recommended)
- *1B model*: 25.7% on BFCL V2 (limited capability)

The 3B model shows significantly better tool calling performance, making it our recommended choice.

* Expansion Opportunities

Priority additions for broader coverage:
1. Command execution (controlled subprocess)
2. Git operations (status, diff, commit)
3. Multi-file operations (bulk edit/rename)
4. Simple web requests (fetch documentation)
#+end_src

* Results and Observations

** Setup Results
- [ ] Ollama installation status
- [ ] Llama 3.2 model availability (both 1B and 3B)
- [ ] SSH tunnel performance

** Test Results
- [ ] Basic tool recognition rate
- [ ] File operation success rate
- [ ] Complex workflow completion
- [ ] Error handling effectiveness

** Performance Measurements
- [ ] Latency per operation type
- [ ] Throughput for multi-tool sequences
- [ ] SSH tunnel overhead
- [ ] Model comparison (3B vs 1B)

** Gap Analysis
- [ ] Missing capabilities vs production tools
- [ ] LLM limitations discovered
- [ ] Integration challenges

* Quick Commands

** Testing with Llama 3.2
#+begin_src bash
# Pull the recommended model
ollama pull llama3.2:3b

# Test basic tool calling
cd experiments/ollama-tool-validation
make run

# Run validation suite
make test

# Compare models
guile3 src/model-comparison.scm
#+end_src

** SSH Tunnel Setup
#+begin_src bash
# From remote machine (e.g., nexushive)
ssh -R 11434:localhost:11434 your-mac.local

# Test connection
curl http://localhost:11434/api/tags
#+end_src

* Conclusions

TBD after experiment completion. Key areas to evaluate:
1. Llama 3.2 3B model reliability for tool calling
2. Performance impact of SSH tunneling
3. Viability for production use cases
4. Comparison with cloud-based solutions
#+end_src